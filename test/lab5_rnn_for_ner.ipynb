{
 "cells": [
  {
   "cell_type": "code",
   "id": "608a3c8b554361e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:32.019231Z",
     "start_time": "2025-11-26T14:40:30.292638Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import time"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dangth2004/Programming/Natural-Language-Processing/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d8c1f8765923a855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:32.151513Z",
     "start_time": "2025-11-26T14:40:32.023985Z"
    }
   },
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 3\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "TAG_PAD_IDX = -1  # Giá trị ignore_index cho Loss function"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8509dfe231298496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:39.031774Z",
     "start_time": "2025-11-26T14:40:32.198541Z"
    }
   },
   "source": [
    "# 1. Tải dữ liệu\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "ner_feature = dataset[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "print(f\"Available tags: {label_names}\")\n",
    "\n",
    "# Xây vocab\n",
    "word_to_ix = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "tag_to_ix = {}\n",
    "\n",
    "# word_to_ix\n",
    "for tokens in dataset[\"train\"][\"tokens\"]:\n",
    "    for word in tokens:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "for idx, label in enumerate(label_names):\n",
    "    tag_to_ix[label] = idx\n",
    "\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "print(f\"Vocab size: {len(word_to_ix)}\")\n",
    "print(f\"Number of tags: {len(tag_to_ix)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tags: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "Vocab size: 23625\n",
      "Number of tags: 9\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "64e55d9def050218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:41.242308Z",
     "start_time": "2025-11-26T14:40:40.951652Z"
    }
   },
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset_split, word_to_ix, tag_to_ix, label_names):\n",
    "        self.sentences = dataset_split[\"tokens\"]\n",
    "        self.ner_tags_ids = dataset_split[\"ner_tags\"]  # Dữ liệu gốc là ID số nguyên\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token_list = self.sentences[idx]\n",
    "        tag_id_list = self.ner_tags_ids[idx]\n",
    "\n",
    "        # Chuyển từ -> index (xử lý UNK)\n",
    "        sentence_indices = [self.word_to_ix.get(w, self.word_to_ix[UNK_TOKEN]) for w in token_list]\n",
    "        tag_indices = [self.tag_to_ix[self.label_names[t_id]] for t_id in tag_id_list]\n",
    "\n",
    "        return torch.tensor(sentence_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "\n",
    "    # Pad sentences với giá trị 0 (index của <PAD>)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word_to_ix[PAD_TOKEN])\n",
    "\n",
    "    # Pad tags với giá trị -1 (TAG_PAD_IDX)\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=TAG_PAD_IDX)\n",
    "\n",
    "    return sentences_padded, tags_padded\n",
    "\n",
    "\n",
    "train_dataset = NERDataset(dataset[\"train\"], word_to_ix, tag_to_ix, label_names)\n",
    "val_dataset = NERDataset(dataset[\"validation\"], word_to_ix, tag_to_ix, label_names)\n",
    "test_dataset = NERDataset(dataset[\"test\"], word_to_ix, tag_to_ix, label_names)  # Optional\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "da6eb1d409ba6fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:41.435631Z",
     "start_time": "2025-11-26T14:40:41.246556Z"
    }
   },
   "source": [
    "class SimpleRNNForTokenClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleRNNForTokenClassification, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_to_ix[PAD_TOKEN])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # -> [batch_size, seq_len, embed_dim]\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        predictions = self.fc(outputs)  # -> [batch_size, seq_len, output_dim]\n",
    "        return predictions\n",
    "\n",
    "\n",
    "model = SimpleRNNForTokenClassification(\n",
    "    vocab_size=len(word_to_ix),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=len(tag_to_ix)\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNNForTokenClassification(\n",
      "  (embedding): Embedding(23625, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 256, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "f22b418f4860ebe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:49.028737Z",
     "start_time": "2025-11-26T14:40:41.441440Z"
    }
   },
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct_tokens = 0\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in dataloader:\n",
    "            sentences, tags = sentences.to(DEVICE), tags.to(DEVICE)\n",
    "\n",
    "            outputs = model(sentences)\n",
    "            loss = criterion(outputs.view(-1, len(tag_to_ix)), tags.view(-1))\n",
    "            predicted_tags = torch.argmax(outputs, dim=-1)\n",
    "            mask = (tags != TAG_PAD_IDX)\n",
    "\n",
    "            # chỉ tại các vị trí mask == True\n",
    "            correct = (predicted_tags == tags) & mask\n",
    "\n",
    "            correct_tokens += correct.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    val_time = end_time - start_time\n",
    "    val_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    return val_loss, accuracy, val_time\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Ignore_index=-1 để bỏ qua padding khi tính loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_tokens = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    for batch_idx, (sentences, tags) in enumerate(dataloader):\n",
    "        sentences, tags = sentences.to(DEVICE), tags.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sentences)  # [batch, seq_len, num_tags]\n",
    "        loss = criterion(predictions.view(-1, len(tag_to_ix)), tags.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_tags = torch.argmax(predictions, dim=-1)\n",
    "        mask = (tags != TAG_PAD_IDX)\n",
    "        correct = (predicted_tags == tags) & mask\n",
    "        correct_tokens += correct.sum().item()\n",
    "\n",
    "        total_tokens += mask.sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    train_time = end_time - start_time\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    train_acc = correct_tokens / total_tokens\n",
    "\n",
    "    return avg_loss, train_acc, train_time\n",
    "\n",
    "\n",
    "# Vòng lặp huấn luyện\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.perf_counter()\n",
    "    avg_loss, train_acc, train_time = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, avg_dev_acc, val_time = evaluate(model, val_loader, criterion)\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | '\n",
    "          f'Train Loss: {avg_loss:.3f} | '\n",
    "          f'Val Loss: {val_loss:.3f} | '\n",
    "          f'Train Acc: {train_acc:.3f} | '\n",
    "          f'Val Acc: {avg_dev_acc:.3f} | '\n",
    "          f'Training time: {train_time:.3f} | '\n",
    "          f'Validation time: {val_time:.3f} | '\n",
    "          f'Total time: {total_time:.3f} | ')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.620 | Val Loss: 0.445 | Train Acc: 0.841 | Val Acc: 0.877 | Training time: 2.207 | Validation time: 0.151 | Total time: 2.358 | \n",
      "Epoch: 02 | Train Loss: 0.324 | Val Loss: 0.291 | Train Acc: 0.903 | Val Acc: 0.916 | Training time: 2.040 | Validation time: 0.144 | Total time: 2.184 | \n",
      "Epoch: 03 | Train Loss: 0.201 | Val Loss: 0.233 | Train Acc: 0.939 | Val Acc: 0.931 | Training time: 1.983 | Validation time: 0.142 | Total time: 2.125 | \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T14:40:49.149635Z",
     "start_time": "2025-11-26T14:40:49.088538Z"
    }
   },
   "source": [
    "def predict_sentence(sentence_str):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = sentence_str.split()\n",
    "\n",
    "    indices = [word_to_ix.get(w, word_to_ix[UNK_TOKEN]) for w in tokens]\n",
    "    tensor_input = torch.tensor([indices], dtype=torch.long).to(DEVICE)  # Batch size = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_input)\n",
    "        predicted_indices = torch.argmax(output, dim=-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for token, idx in zip(tokens, predicted_indices):\n",
    "        label = ix_to_tag[idx]\n",
    "        results.append((token, label))\n",
    "\n",
    "    print(f\"\\nSentence: {sentence_str}\")\n",
    "    print(f\"{'Token':<15} {'Predicted Label'}\")\n",
    "    print(\"-\" * 30)\n",
    "    for token, label in results:\n",
    "        print(f\"{token:<15} {label}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "sample_sentence = \"VNU University of Science is located in Hanoi\"\n",
    "predict_sentence(sample_sentence)\n",
    "\n",
    "sample_sentence_2 = \"WHO is an organization based in Geneva\"\n",
    "predict_sentence(sample_sentence_2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: VNU University of Science is located in Hanoi\n",
      "Token           Predicted Label\n",
      "------------------------------\n",
      "VNU             B-ORG\n",
      "University      I-ORG\n",
      "of              I-ORG\n",
      "Science         I-ORG\n",
      "is              O\n",
      "located         O\n",
      "in              O\n",
      "Hanoi           O\n",
      "\n",
      "Sentence: WHO is an organization based in Geneva\n",
      "Token           Predicted Label\n",
      "------------------------------\n",
      "WHO             O\n",
      "is              O\n",
      "an              O\n",
      "organization    O\n",
      "based           O\n",
      "in              O\n",
      "Geneva          B-LOC\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
